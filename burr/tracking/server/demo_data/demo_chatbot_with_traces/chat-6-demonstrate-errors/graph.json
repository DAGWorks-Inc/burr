{"type": "application", "entrypoint": "prompt", "actions": [{"type": "action", "name": "prompt", "reads": [], "writes": ["chat_history", "prompt"], "code": "@action(reads=[], writes=[\"chat_history\", \"prompt\"])\ndef process_prompt(state: State, prompt: str, __tracer: TracerFactory) -> Tuple[dict, State]:\n    with __tracer(\"process_prompt\") as tracer:\n        result = {\"chat_item\": {\"role\": \"user\", \"content\": prompt, \"type\": \"text\"}}\n        tracer.log_attributes(prompt=prompt)\n    return result, state.wipe(keep=[\"prompt\", \"chat_history\"]).append(\n        chat_history=result[\"chat_item\"]\n    ).update(prompt=prompt)\n", "inputs": [], "optional_inputs": ["__tracer", "prompt"]}, {"type": "action", "name": "check_safety", "reads": ["prompt"], "writes": ["safe"], "code": "@action(reads=[\"prompt\"], writes=[\"safe\"])\ndef check_safety(state: State, __tracer: TracerFactory) -> Tuple[dict, State]:\n    with __tracer(\"check_safety\"):\n        result = {\"safe\": \"unsafe\" not in state[\"prompt\"]}  # quick hack to demonstrate\n    return result, state.update(safe=result[\"safe\"])\n", "inputs": [], "optional_inputs": ["__tracer"]}, {"type": "action", "name": "decide_mode", "reads": ["prompt"], "writes": ["mode"], "code": "@action(reads=[\"prompt\"], writes=[\"mode\"])\ndef choose_mode(state: State, __tracer: TracerFactory) -> Tuple[dict, State]:\n    with __tracer(\"generate_prompt\"):\n        prompt = (\n            f\"You are a chatbot. You've been prompted this: {state['prompt']}. \"\n            f\"You have the capability of responding in the following modes: {', '.join(MODES)}. \"\n            \"Please respond with *only* a single word representing the mode that most accurately \"\n            \"corresponds to the prompt. Fr instance, if the prompt is 'draw a picture of a cat', \"\n            \"the mode would be 'generate_image'. If the prompt is 'what is the capital of France', the mode would be 'answer_question'.\"\n            \"If none of these modes apply, please respond with 'unknown'.\"\n        )\n    with __tracer(\"query_openai\", span_dependencies=[\"generate_prompt\"]):\n        with __tracer(\"create_openai_client\"):\n            client = _get_openai_client()\n        with __tracer(\"query_openai\") as tracer:\n            result = client.chat.completions.create(\n                model=\"gpt-4\",\n                messages=[\n                    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n                    {\"role\": \"user\", \"content\": prompt},\n                ],\n            )\n            tracer.log_attributes(\n                response=result.choices[0].message.content,\n                prompt_tokens=result.usage.prompt_tokens,\n                total_tokens=result.usage.total_tokens,\n                completion_tokens=result.usage.completion_tokens,\n            )\n    with __tracer(\"process_openai_response\", span_dependencies=[\"query_openai\"]):\n        content = result.choices[0].message.content\n        mode = content.lower()\n        if mode not in MODES:\n            mode = \"unknown\"\n        result = {\"mode\": mode}\n    return result, state.update(**result)\n", "inputs": [], "optional_inputs": ["__tracer"]}, {"type": "action", "name": "generate_image", "reads": ["prompt", "chat_history", "mode"], "writes": ["response"], "code": "@action(reads=[\"prompt\", \"chat_history\", \"mode\"], writes=[\"response\"])\ndef image_response(\n    state: State, __tracer: TracerFactory, model: str = \"dall-e-2\"\n) -> Tuple[dict, State]:\n    __tracer.log_attributes(model=model)\n    with __tracer(\"create_openai_client\"):\n        client = _get_openai_client()\n    with __tracer(\"query_openai_image\", span_dependencies=[\"create_openai_client\"]):\n        result = client.images.generate(\n            model=model, prompt=state[\"prompt\"], size=\"1024x1024\", quality=\"standard\", n=1\n        )\n        response = result.data[0].url\n    with __tracer(\"process_openai_response\", span_dependencies=[\"query_openai_image\"]):\n        result = {\n            \"response\": {\"content\": response, \"type\": MODES[state[\"mode\"]], \"role\": \"assistant\"}\n        }\n        __tracer.log_attributes(response=response)\n    return result, state.update(**result)\n", "inputs": ["model"], "optional_inputs": ["__tracer"]}, {"type": "action", "name": "generate_code", "reads": ["prompt", "chat_history", "mode"], "writes": ["response"], "code": "@action(reads=[\"prompt\", \"chat_history\", \"mode\"], writes=[\"response\"])\ndef chat_response(\n    state: State,\n    prepend_prompt: str,\n    __tracer: TracerFactory,\n    model: str = \"gpt-3.5-turbo\",\n) -> Tuple[dict, State]:\n    __tracer.log_attributes(model=model, prepend_prompt=prepend_prompt)\n    with __tracer(\"process_chat_history\"):\n        chat_history = state[\"chat_history\"].copy()\n        chat_history[-1][\"content\"] = f\"{prepend_prompt}: {chat_history[-1]['content']}\"\n        chat_history_api_format = [\n            {\n                \"role\": chat[\"role\"],\n                \"content\": chat[\"content\"],\n            }\n            for chat in chat_history\n        ]\n    with __tracer(\"query_openai\", span_dependencies=[\"change_chat_history\"]):\n        with __tracer(\"create_openai_client\"):\n            client = _get_openai_client()\n        with __tracer(\"query_openai\", span_dependencies=[\"create_openai_client\"]) as tracer:\n            result = client.chat.completions.create(\n                model=model,\n                messages=chat_history_api_format,\n            )\n            tracer.log_attributes(\n                response=result.choices[0].message.content,\n                prompt_tokens=result.usage.prompt_tokens,\n                total_tokens=result.usage.total_tokens,\n                completion_tokens=result.usage.completion_tokens,\n            )\n    with __tracer(\"process_openai_response\", span_dependencies=[\"query_openai\"]):\n        response = result.choices[0].message.content\n        result = {\n            \"response\": {\"content\": response, \"type\": MODES[state[\"mode\"]], \"role\": \"assistant\"}\n        }\n    return result, state.update(**result)\n", "inputs": ["model"], "optional_inputs": ["__tracer"]}, {"type": "action", "name": "answer_question", "reads": ["prompt", "chat_history", "mode"], "writes": ["response"], "code": "@action(reads=[\"prompt\", \"chat_history\", \"mode\"], writes=[\"response\"])\ndef chat_response(\n    state: State,\n    prepend_prompt: str,\n    __tracer: TracerFactory,\n    model: str = \"gpt-3.5-turbo\",\n) -> Tuple[dict, State]:\n    __tracer.log_attributes(model=model, prepend_prompt=prepend_prompt)\n    with __tracer(\"process_chat_history\"):\n        chat_history = state[\"chat_history\"].copy()\n        chat_history[-1][\"content\"] = f\"{prepend_prompt}: {chat_history[-1]['content']}\"\n        chat_history_api_format = [\n            {\n                \"role\": chat[\"role\"],\n                \"content\": chat[\"content\"],\n            }\n            for chat in chat_history\n        ]\n    with __tracer(\"query_openai\", span_dependencies=[\"change_chat_history\"]):\n        with __tracer(\"create_openai_client\"):\n            client = _get_openai_client()\n        with __tracer(\"query_openai\", span_dependencies=[\"create_openai_client\"]) as tracer:\n            result = client.chat.completions.create(\n                model=model,\n                messages=chat_history_api_format,\n            )\n            tracer.log_attributes(\n                response=result.choices[0].message.content,\n                prompt_tokens=result.usage.prompt_tokens,\n                total_tokens=result.usage.total_tokens,\n                completion_tokens=result.usage.completion_tokens,\n            )\n    with __tracer(\"process_openai_response\", span_dependencies=[\"query_openai\"]):\n        response = result.choices[0].message.content\n        result = {\n            \"response\": {\"content\": response, \"type\": MODES[state[\"mode\"]], \"role\": \"assistant\"}\n        }\n    return result, state.update(**result)\n", "inputs": ["model"], "optional_inputs": ["__tracer"]}, {"type": "action", "name": "prompt_for_more", "reads": ["prompt", "chat_history"], "writes": ["response"], "code": "@action(reads=[\"prompt\", \"chat_history\"], writes=[\"response\"])\ndef prompt_for_more(state: State) -> Tuple[dict, State]:\n    result = {\n        \"response\": {\n            \"content\": \"None of the response modes I support apply to your question. Please clarify?\",\n            \"type\": \"text\",\n            \"role\": \"assistant\",\n        }\n    }\n    return result, state.update(**result)\n", "inputs": [], "optional_inputs": []}, {"type": "action", "name": "response", "reads": ["response", "safe", "mode"], "writes": ["chat_history"], "code": "@action(reads=[\"response\", \"safe\", \"mode\"], writes=[\"chat_history\"])\ndef response(state: State, __tracer: TracerFactory) -> Tuple[dict, State]:\n    with __tracer(\"process_response\"):\n        if not state[\"safe\"]:\n            with __tracer(\"unsafe\"):\n                result = {\n                    \"chat_item\": {\n                        \"role\": \"assistant\",\n                        \"content\": \"I'm sorry, I can't respond to that.\",\n                        \"type\": \"text\",\n                    }\n                }\n        else:\n            with __tracer(\"safe\"):\n                result = {\"chat_item\": state[\"response\"]}\n    return result, state.append(chat_history=result[\"chat_item\"])\n", "inputs": [], "optional_inputs": ["__tracer"]}], "transitions": [{"type": "transition", "from_": "prompt", "to": "check_safety", "condition": "default"}, {"type": "transition", "from_": "check_safety", "to": "decide_mode", "condition": "safe=True"}, {"type": "transition", "from_": "check_safety", "to": "response", "condition": "default"}, {"type": "transition", "from_": "decide_mode", "to": "generate_image", "condition": "mode=generate_image"}, {"type": "transition", "from_": "decide_mode", "to": "generate_code", "condition": "mode=generate_code"}, {"type": "transition", "from_": "decide_mode", "to": "answer_question", "condition": "mode=answer_question"}, {"type": "transition", "from_": "decide_mode", "to": "prompt_for_more", "condition": "default"}, {"type": "transition", "from_": "generate_image", "to": "response", "condition": "default"}, {"type": "transition", "from_": "answer_question", "to": "response", "condition": "default"}, {"type": "transition", "from_": "generate_code", "to": "response", "condition": "default"}, {"type": "transition", "from_": "prompt_for_more", "to": "response", "condition": "default"}, {"type": "transition", "from_": "response", "to": "prompt", "condition": "default"}]}