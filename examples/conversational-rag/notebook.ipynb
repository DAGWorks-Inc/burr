{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1622e1563a35aa32",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Conversational RAG Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9069a73d207fd136",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "!pip install burr[start] sf-hamilton[visualization] openai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T21:02:28.908380Z",
     "start_time": "2024-05-10T21:02:18.236814Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import pprint\n",
    "from typing import Tuple\n",
    "from hamilton import dataflows, driver\n",
    "import burr.core\n",
    "from burr.core import ApplicationBuilder, State, default, expr\n",
    "from burr.core.action import action\n",
    "from burr.tracking import LocalTrackingClient\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1d578469a918b1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Load your \"chain\" or conversational RAG \"pipeline\"\n",
    "\n",
    "We use Hamilton here. But you could use LangChain, etc., or forgo them and write your own code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6a018aff1154f0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T20:47:42.057246Z",
     "start_time": "2024-03-26T20:47:40.500075Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 10.0.1 (20240210.2158)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"1051pt\" height=\"343pt\"\n",
       " viewBox=\"0.00 0.00 1050.60 342.80\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 338.8)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-338.8 1046.6,-338.8 1046.6,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster__legend</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"black\" points=\"19.38,-196.8 19.38,-326.8 104.23,-326.8 104.23,-196.8 19.38,-196.8\"/>\n",
       "<text text-anchor=\"middle\" x=\"61.8\" y=\"-309.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Legend</text>\n",
       "</g>\n",
       "<!-- vector_store -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>vector_store</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M533.05,-63.6C533.05,-63.6 409.45,-63.6 409.45,-63.6 403.45,-63.6 397.45,-57.6 397.45,-51.6 397.45,-51.6 397.45,-12 397.45,-12 397.45,-6 403.45,0 409.45,0 409.45,0 533.05,0 533.05,0 539.05,0 545.05,-6 545.05,-12 545.05,-12 545.05,-51.6 545.05,-51.6 545.05,-57.6 539.05,-63.6 533.05,-63.6\"/>\n",
       "<text text-anchor=\"start\" x=\"430.75\" y=\"-40.5\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\">vector_store</text>\n",
       "<text text-anchor=\"start\" x=\"408.25\" y=\"-12.5\" font-family=\"Helvetica,sans-Serif\" font-style=\"italic\" font-size=\"14.00\">VectorStoreRetriever</text>\n",
       "</g>\n",
       "<!-- context -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>context</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M636.15,-141.6C636.15,-141.6 590.55,-141.6 590.55,-141.6 584.55,-141.6 578.55,-135.6 578.55,-129.6 578.55,-129.6 578.55,-90 578.55,-90 578.55,-84 584.55,-78 590.55,-78 590.55,-78 636.15,-78 636.15,-78 642.15,-78 648.15,-84 648.15,-90 648.15,-90 648.15,-129.6 648.15,-129.6 648.15,-135.6 642.15,-141.6 636.15,-141.6\"/>\n",
       "<text text-anchor=\"start\" x=\"589.35\" y=\"-118.5\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\">context</text>\n",
       "<text text-anchor=\"start\" x=\"605.85\" y=\"-90.5\" font-family=\"Helvetica,sans-Serif\" font-style=\"italic\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- vector_store&#45;&gt;context -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>vector_store&#45;&gt;context</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M533.57,-64C538.99,-66.94 544.38,-69.91 549.55,-72.8 555.71,-76.25 562.17,-79.95 568.5,-83.63\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"566.58,-86.56 576.98,-88.59 570.12,-80.52 566.58,-86.56\"/>\n",
       "</g>\n",
       "<!-- answer_prompt -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>answer_prompt</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M788.75,-207.6C788.75,-207.6 689.15,-207.6 689.15,-207.6 683.15,-207.6 677.15,-201.6 677.15,-195.6 677.15,-195.6 677.15,-156 677.15,-156 677.15,-150 683.15,-144 689.15,-144 689.15,-144 788.75,-144 788.75,-144 794.75,-144 800.75,-150 800.75,-156 800.75,-156 800.75,-195.6 800.75,-195.6 800.75,-201.6 794.75,-207.6 788.75,-207.6\"/>\n",
       "<text text-anchor=\"start\" x=\"687.95\" y=\"-184.5\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\">answer_prompt</text>\n",
       "<text text-anchor=\"start\" x=\"731.45\" y=\"-156.5\" font-family=\"Helvetica,sans-Serif\" font-style=\"italic\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- context&#45;&gt;answer_prompt -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>context&#45;&gt;answer_prompt</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M648.45,-128.01C654.6,-131.29 661.2,-134.82 667.94,-138.42\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"666.06,-141.38 676.53,-143.01 669.36,-135.21 666.06,-141.38\"/>\n",
       "</g>\n",
       "<!-- conversational_rag_response -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>conversational_rag_response</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M1030.6,-237.6C1030.6,-237.6 841.75,-237.6 841.75,-237.6 835.75,-237.6 829.75,-231.6 829.75,-225.6 829.75,-225.6 829.75,-186 829.75,-186 829.75,-180 835.75,-174 841.75,-174 841.75,-174 1030.6,-174 1030.6,-174 1036.6,-174 1042.6,-180 1042.6,-186 1042.6,-186 1042.6,-225.6 1042.6,-225.6 1042.6,-231.6 1036.6,-237.6 1030.6,-237.6\"/>\n",
       "<text text-anchor=\"start\" x=\"840.55\" y=\"-214.5\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\">conversational_rag_response</text>\n",
       "<text text-anchor=\"start\" x=\"928.68\" y=\"-186.5\" font-family=\"Helvetica,sans-Serif\" font-style=\"italic\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- standalone_question -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>standalone_question</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M537.55,-207.6C537.55,-207.6 404.95,-207.6 404.95,-207.6 398.95,-207.6 392.95,-201.6 392.95,-195.6 392.95,-195.6 392.95,-156 392.95,-156 392.95,-150 398.95,-144 404.95,-144 404.95,-144 537.55,-144 537.55,-144 543.55,-144 549.55,-150 549.55,-156 549.55,-156 549.55,-195.6 549.55,-195.6 549.55,-201.6 543.55,-207.6 537.55,-207.6\"/>\n",
       "<text text-anchor=\"start\" x=\"403.75\" y=\"-184.5\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\">standalone_question</text>\n",
       "<text text-anchor=\"start\" x=\"463.75\" y=\"-156.5\" font-family=\"Helvetica,sans-Serif\" font-style=\"italic\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- standalone_question&#45;&gt;context -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>standalone_question&#45;&gt;context</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M540.66,-143.57C549.94,-139.2 559.26,-134.81 567.97,-130.71\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"569.32,-133.94 576.88,-126.51 566.34,-127.61 569.32,-133.94\"/>\n",
       "</g>\n",
       "<!-- standalone_question&#45;&gt;answer_prompt -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>standalone_question&#45;&gt;answer_prompt</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M549.82,-175.8C586.3,-175.8 629.57,-175.8 665.4,-175.8\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"665.24,-179.3 675.24,-175.8 665.24,-172.3 665.24,-179.3\"/>\n",
       "</g>\n",
       "<!-- standalone_question_prompt -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>standalone_question_prompt</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M351.95,-185.6C351.95,-185.6 164.6,-185.6 164.6,-185.6 158.6,-185.6 152.6,-179.6 152.6,-173.6 152.6,-173.6 152.6,-134 152.6,-134 152.6,-128 158.6,-122 164.6,-122 164.6,-122 351.95,-122 351.95,-122 357.95,-122 363.95,-128 363.95,-134 363.95,-134 363.95,-173.6 363.95,-173.6 363.95,-179.6 357.95,-185.6 351.95,-185.6\"/>\n",
       "<text text-anchor=\"start\" x=\"163.4\" y=\"-162.5\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\">standalone_question_prompt</text>\n",
       "<text text-anchor=\"start\" x=\"250.78\" y=\"-134.5\" font-family=\"Helvetica,sans-Serif\" font-style=\"italic\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- standalone_question_prompt&#45;&gt;standalone_question -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>standalone_question_prompt&#45;&gt;standalone_question</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M364.14,-164.74C370.01,-165.35 375.87,-165.96 381.66,-166.56\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"380.88,-170 391.19,-167.56 381.61,-163.04 380.88,-170\"/>\n",
       "</g>\n",
       "<!-- answer_prompt&#45;&gt;conversational_rag_response -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>answer_prompt&#45;&gt;conversational_rag_response</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M801.18,-185.21C806.7,-186.06 812.4,-186.93 818.2,-187.82\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"817.53,-191.26 827.94,-189.32 818.59,-184.34 817.53,-191.26\"/>\n",
       "</g>\n",
       "<!-- llm_client -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>llm_client</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M288.58,-267.6C288.58,-267.6 227.98,-267.6 227.98,-267.6 221.98,-267.6 215.98,-261.6 215.98,-255.6 215.98,-255.6 215.98,-216 215.98,-216 215.98,-210 221.98,-204 227.98,-204 227.98,-204 288.58,-204 288.58,-204 294.58,-204 300.58,-210 300.58,-216 300.58,-216 300.58,-255.6 300.58,-255.6 300.58,-261.6 294.58,-267.6 288.58,-267.6\"/>\n",
       "<text text-anchor=\"start\" x=\"226.78\" y=\"-244.5\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\">llm_client</text>\n",
       "<text text-anchor=\"start\" x=\"235.03\" y=\"-216.5\" font-family=\"Helvetica,sans-Serif\" font-style=\"italic\" font-size=\"14.00\">OpenAI</text>\n",
       "</g>\n",
       "<!-- llm_client&#45;&gt;conversational_rag_response -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>llm_client&#45;&gt;conversational_rag_response</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M300.74,-235C391.46,-233.12 614.35,-227.69 800.75,-216.8 806.45,-216.47 812.28,-216.1 818.17,-215.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"818.3,-219.2 828.03,-215.02 817.81,-212.22 818.3,-219.2\"/>\n",
       "</g>\n",
       "<!-- llm_client&#45;&gt;standalone_question -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>llm_client&#45;&gt;standalone_question</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M300.96,-223.95C324.07,-217.37 353.72,-208.94 381.74,-200.97\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"382.67,-204.35 391.33,-198.24 380.76,-197.61 382.67,-204.35\"/>\n",
       "</g>\n",
       "<!-- _vector_store_inputs -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>_vector_store_inputs</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"316.95,-54.1 199.6,-54.1 199.6,-9.5 316.95,-9.5 316.95,-54.1\"/>\n",
       "<text text-anchor=\"start\" x=\"214.4\" y=\"-26\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">input_texts</text>\n",
       "<text text-anchor=\"start\" x=\"285.65\" y=\"-26\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">list</text>\n",
       "</g>\n",
       "<!-- _vector_store_inputs&#45;&gt;vector_store -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>_vector_store_inputs&#45;&gt;vector_store</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M317.2,-31.8C338.28,-31.8 362.57,-31.8 385.49,-31.8\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"385.46,-35.3 395.46,-31.8 385.46,-28.3 385.46,-35.3\"/>\n",
       "</g>\n",
       "<!-- _context_inputs -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>_context_inputs</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"512.68,-126.1 429.83,-126.1 429.83,-81.5 512.68,-81.5 512.68,-126.1\"/>\n",
       "<text text-anchor=\"start\" x=\"444.63\" y=\"-98\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">top_k</text>\n",
       "<text text-anchor=\"start\" x=\"483.63\" y=\"-98\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">int</text>\n",
       "</g>\n",
       "<!-- _context_inputs&#45;&gt;context -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>_context_inputs&#45;&gt;context</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M513.15,-105.55C530.11,-106.28 549.85,-107.12 567.23,-107.87\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"566.6,-111.34 576.74,-108.28 566.9,-104.35 566.6,-111.34\"/>\n",
       "</g>\n",
       "<!-- _standalone_question_prompt_inputs -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>_standalone_question_prompt_inputs</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"123.6,-186.6 0,-186.6 0,-121 123.6,-121 123.6,-186.6\"/>\n",
       "<text text-anchor=\"start\" x=\"25.3\" y=\"-158.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">question</text>\n",
       "<text text-anchor=\"start\" x=\"93.3\" y=\"-158.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">str</text>\n",
       "<text text-anchor=\"start\" x=\"14.43\" y=\"-137.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">chat_history</text>\n",
       "<text text-anchor=\"start\" x=\"92.55\" y=\"-137.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">list</text>\n",
       "</g>\n",
       "<!-- _standalone_question_prompt_inputs&#45;&gt;standalone_question_prompt -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>_standalone_question_prompt_inputs&#45;&gt;standalone_question_prompt</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M124.07,-153.8C129.56,-153.8 135.24,-153.8 141,-153.8\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"140.67,-157.3 150.67,-153.8 140.67,-150.3 140.67,-157.3\"/>\n",
       "</g>\n",
       "<!-- input -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>input</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"88.8,-241.1 34.8,-241.1 34.8,-204.5 88.8,-204.5 88.8,-241.1\"/>\n",
       "<text text-anchor=\"middle\" x=\"61.8\" y=\"-217\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">input</text>\n",
       "</g>\n",
       "<!-- function -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>function</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M84.23,-296.1C84.23,-296.1 39.38,-296.1 39.38,-296.1 33.38,-296.1 27.38,-290.1 27.38,-284.1 27.38,-284.1 27.38,-271.5 27.38,-271.5 27.38,-265.5 33.38,-259.5 39.38,-259.5 39.38,-259.5 84.23,-259.5 84.23,-259.5 90.23,-259.5 96.23,-265.5 96.23,-271.5 96.23,-271.5 96.23,-284.1 96.23,-284.1 96.23,-290.1 90.23,-296.1 84.23,-296.1\"/>\n",
       "<text text-anchor=\"middle\" x=\"61.8\" y=\"-272\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">function</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x1255f0940>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loads Hamilton DAG\n",
    "conversational_rag = dataflows.import_module(\"conversational_rag\")\n",
    "conversational_rag_driver = (\n",
    "    driver.Builder()\n",
    "    .with_config({})  # replace with configuration as appropriate\n",
    "    .with_modules(conversational_rag)\n",
    "    .build()\n",
    ")\n",
    "conversational_rag_driver.display_all_functions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b3515afd2de6e4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Create the actions that will constitute our application\n",
    "\n",
    "We will use the functional (vs class) approach to declaring actions here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6433dad5abc6eb16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T20:47:42.065785Z",
     "start_time": "2024-03-26T20:47:42.059539Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "@action(\n",
    "    reads=[\"question\", \"chat_history\"],\n",
    "    writes=[\"chat_history\"],\n",
    ")\n",
    "def ai_converse(state: State, vector_store: object) -> Tuple[dict, State]:\n",
    "    \"\"\"AI conversing step. Uses Hamilton to execute the conversational pipeline.\"\"\"\n",
    "    result = conversational_rag_driver.execute(\n",
    "        [\"conversational_rag_response\"],\n",
    "        inputs={\n",
    "            \"question\": state[\"question\"],\n",
    "            \"chat_history\": state[\"chat_history\"],\n",
    "        },\n",
    "        # we use overrides here because we want to pass in the vector store\n",
    "        overrides={\n",
    "            \"vector_store\": vector_store,\n",
    "        }\n",
    "    )\n",
    "    new_history = f\"AI: {result['conversational_rag_response']}\"\n",
    "    return result, state.append(chat_history=new_history)\n",
    "\n",
    "\n",
    "@action(\n",
    "    reads=[],\n",
    "    writes=[\"question\", \"chat_history\"],\n",
    ")\n",
    "def human_converse(state: State, user_question: str) -> Tuple[dict, State]:\n",
    "    \"\"\"Human converse step -- make sure we get input, and store it as state.\"\"\"\n",
    "    state = state.update(question=user_question).append(chat_history=f\"Human: {user_question}\")\n",
    "    return {\"question\": user_question}, state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4439e0dd39441414",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Add a hook to print the steps -- optional but shows that Burr is pluggable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3c3af84fab14f39",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from burr.core import Action\n",
    "from burr.lifecycle import PostRunStepHook, PreRunStepHook\n",
    "\n",
    "class PrintStepHook(PostRunStepHook, PreRunStepHook):\n",
    "    \"\"\"Custom hook to print the action/result after each step.\"\"\"\n",
    "\n",
    "    def pre_run_step(self, action: Action, **future_kwargs):\n",
    "        if action.name == \"ai_converse\":\n",
    "            print(\"🤔 AI is thinking...\")\n",
    "        if action.name == \"human_converse\":\n",
    "            print(\"⏳Processing input from user...\")\n",
    "\n",
    "    def post_run_step(self, *, state: \"State\", action: Action, result: dict, **future_kwargs):\n",
    "        if action.name == \"human_converse\":\n",
    "            print(\"🎙💬\", result[\"question\"], \"\\n\")\n",
    "        if action.name == \"ai_converse\":\n",
    "            print(\"🤖💬\", result[\"conversational_rag_response\"], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9579b47aac2c53a0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Create the application\n",
    "\n",
    "We now create the application, which is a collection of actions, and then set the transitions between the actions based on values in State.\n",
    "\n",
    "We also intialize initial values etc to populate the application with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77e9f67b660a0953",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T20:50:25.642660Z",
     "start_time": "2024-03-26T20:50:25.616245Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# what we will do RAG over.\n",
    "initial_documents = [\n",
    "    \"harrison worked at kensho\",\n",
    "    \"stefan worked at Stitch Fix\",\n",
    "    \"stefan likes tacos\",\n",
    "    \"elijah worked at TwoSigma\",\n",
    "    \"elijah likes mango\",\n",
    "    \"stefan used to work at IBM\",\n",
    "    \"elijah likes to go biking\",\n",
    "    \"stefan likes to bake sourdough\",\n",
    "]\n",
    "# bootstrap the vector store;\n",
    "vector_store = conversational_rag_driver.execute(\n",
    "    [\"vector_store\"],\n",
    "    inputs={\"input_texts\": initial_documents})[\"vector_store\"]\n",
    "# what we will initialize the application with\n",
    "initial_state = {\n",
    "    \"question\": \"\",\n",
    "    \"chat_history\": [],\n",
    "}\n",
    "\n",
    "app_id = str(uuid.uuid4())\n",
    "app = (\n",
    "    ApplicationBuilder()\n",
    "    # add the actions\n",
    "    .with_actions(\n",
    "        # bind the vector store to the AI conversational step\n",
    "        ai_converse=ai_converse.bind(vector_store=vector_store),\n",
    "        human_converse=human_converse,\n",
    "        terminal=burr.core.Result(\"chat_history\"),\n",
    "    )\n",
    "    # set the transitions between actions\n",
    "    .with_transitions(\n",
    "        (\"ai_converse\", \"human_converse\", default),\n",
    "        (\"human_converse\", \"terminal\", expr(\"'exit' in question\")),\n",
    "        (\"human_converse\", \"ai_converse\", default),\n",
    "    )\n",
    "    # add identifiers that will help track the application\n",
    "    .with_identifiers(app_id=app_id, partition_key=\"sample_user\")\n",
    "    # initialize the state\n",
    "    .with_state(**initial_state)\n",
    "    # say what the initial action is\n",
    "    .with_entrypoint(\"human_converse\")\n",
    "    # add a hook to print the steps -- optional but shows that Burr is pluggable\n",
    "    .with_hooks(PrintStepHook())\n",
    "    # add tracking -- this will show up in the BURR UI.\n",
    "    .with_tracker(project=\"demo:conversational-rag\")\n",
    "    # build the application\n",
    "    .build()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf5d1e084a791fa5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T20:48:39.379712Z",
     "start_time": "2024-03-26T20:48:38.701659Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 10.0.1 (20240210.2158)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"292pt\" height=\"194pt\"\n",
       " viewBox=\"0.00 0.00 292.28 193.50\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 189.5)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-189.5 288.28,-189.5 288.28,4 -4,4\"/>\n",
       "<!-- ai_converse -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>ai_converse</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M82.38,-185.5C82.38,-185.5 25.88,-185.5 25.88,-185.5 19.88,-185.5 13.88,-179.5 13.88,-173.5 13.88,-173.5 13.88,-161.5 13.88,-161.5 13.88,-155.5 19.88,-149.5 25.88,-149.5 25.88,-149.5 82.38,-149.5 82.38,-149.5 88.38,-149.5 94.38,-155.5 94.38,-161.5 94.38,-161.5 94.38,-173.5 94.38,-173.5 94.38,-179.5 88.38,-185.5 82.38,-185.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"54.12\" y=\"-162.45\" font-family=\"Times,serif\" font-size=\"14.00\">ai_converse</text>\n",
       "</g>\n",
       "<!-- human_converse -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>human_converse</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M96.25,-118.5C96.25,-118.5 12,-118.5 12,-118.5 6,-118.5 0,-112.5 0,-106.5 0,-106.5 0,-94.5 0,-94.5 0,-88.5 6,-82.5 12,-82.5 12,-82.5 96.25,-82.5 96.25,-82.5 102.25,-82.5 108.25,-88.5 108.25,-94.5 108.25,-94.5 108.25,-106.5 108.25,-106.5 108.25,-112.5 102.25,-118.5 96.25,-118.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"54.12\" y=\"-95.45\" font-family=\"Times,serif\" font-size=\"14.00\">human_converse</text>\n",
       "</g>\n",
       "<!-- ai_converse&#45;&gt;human_converse -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>ai_converse&#45;&gt;human_converse</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M48.07,-149.08C47.49,-143.25 47.27,-136.59 47.42,-130.14\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"50.89,-130.66 47.98,-120.48 43.9,-130.26 50.89,-130.66\"/>\n",
       "</g>\n",
       "<!-- human_converse&#45;&gt;ai_converse -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>human_converse&#45;&gt;ai_converse</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M60.19,-118.97C60.76,-124.8 60.98,-131.46 60.83,-137.92\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"57.36,-137.38 60.27,-147.57 64.34,-137.79 57.36,-137.38\"/>\n",
       "</g>\n",
       "<!-- terminal -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>terminal</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M73,-36C73,-36 35.25,-36 35.25,-36 29.25,-36 23.25,-30 23.25,-24 23.25,-24 23.25,-12 23.25,-12 23.25,-6 29.25,0 35.25,0 35.25,0 73,0 73,0 79,0 85,-6 85,-12 85,-12 85,-24 85,-24 85,-30 79,-36 73,-36\"/>\n",
       "<text text-anchor=\"middle\" x=\"54.12\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">terminal</text>\n",
       "</g>\n",
       "<!-- human_converse&#45;&gt;terminal -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>human_converse&#45;&gt;terminal</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" d=\"M54.12,-82.03C54.12,-72.01 54.12,-59.18 54.12,-47.71\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"57.63,-47.95 54.13,-37.95 50.63,-47.95 57.63,-47.95\"/>\n",
       "<text text-anchor=\"middle\" x=\"98.38\" y=\"-54.2\" font-family=\"Times,serif\" font-size=\"14.00\">&#39;exit&#39; in question</text>\n",
       "</g>\n",
       "<!-- input__user_question -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>input__user_question</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"198.12\" cy=\"-167.5\" rx=\"86.15\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"198.12\" y=\"-162.45\" font-family=\"Times,serif\" font-size=\"14.00\">input: user_question</text>\n",
       "</g>\n",
       "<!-- input__user_question&#45;&gt;human_converse -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>input__user_question&#45;&gt;human_converse</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M162.9,-150.6C144.9,-142.48 122.71,-132.46 103.08,-123.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"104.72,-120.5 94.16,-119.57 101.84,-126.88 104.72,-120.5\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x130bc5ff0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's visualize what we have\n",
    "app.visualize(include_conditions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430bab287b6ad9a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Let's run the app. \n",
    "\n",
    "Let's run it a step at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bcfe9ca48f87618",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T20:50:38.782857Z",
     "start_time": "2024-03-26T20:50:28.797204Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask something (or type exit to quit):  Who is Stefan? Please answer in English.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳Processing input from user...\n",
      "🎙💬 Who is Stefan? Please answer in English. \n",
      "\n",
      "Ran action human_converse with result:\n",
      " {'question': 'Who is Stefan? Please answer in English.'} \n",
      " and state:\n",
      " {'__PRIOR_STEP': 'human_converse',\n",
      " '__SEQUENCE_ID': 0,\n",
      " 'chat_history': ['Human: Who is Stefan? Please answer in English.'],\n",
      " 'question': 'Who is Stefan? Please answer in English.'}\n"
     ]
    }
   ],
   "source": [
    "app.reset_to_entrypoint() # reset the app to the entrypoint\n",
    "user_question = input(\"Ask something (or type exit to quit): \")\n",
    "previous_action, result, state = app.step(\n",
    "    inputs={\"user_question\": user_question},\n",
    ")\n",
    "print(f\"Ran action {previous_action.name} with result:\\n {pprint.pformat(result)} \\n and state:\\n {pprint.pformat(state.get_all())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81940578d58fd602",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T20:50:44.662755Z",
     "start_time": "2024-03-26T20:50:41.919782Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤔 AI is thinking...\n",
      "🤖💬 Stefan is someone who likes tacos, used to work at IBM, worked at Stitch Fix, and likes to bake sourdough. \n",
      "\n",
      "Ran action ai_converse with result:\n",
      " {'conversational_rag_response': 'Stefan is someone who likes tacos, used to '\n",
      "                                'work at IBM, worked at Stitch Fix, and likes '\n",
      "                                'to bake sourdough.'} \n",
      " and state:\n",
      " {'__PRIOR_STEP': 'ai_converse',\n",
      " '__SEQUENCE_ID': 1,\n",
      " 'chat_history': ['Human: Who is Stefan? Please answer in English.',\n",
      "                  'AI: Stefan is someone who likes tacos, used to work at IBM, '\n",
      "                  'worked at Stitch Fix, and likes to bake sourdough.'],\n",
      " 'question': 'Who is Stefan? Please answer in English.'}\n"
     ]
    }
   ],
   "source": [
    "# now let's run the AI conversational step\n",
    "previous_action, result, state = app.step()\n",
    "print(f\"Ran action {previous_action.name} with result:\\n {pprint.pformat(result)} \\n and state:\\n {pprint.pformat(state.get_all())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ec2f4908c2dde2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Let's now run the app to completion\n",
    "\n",
    "You could do the above for each action. Or you could tell the app to run until certain\n",
    "actions/conditions are met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be6c573158b65cb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T20:52:21.364028Z",
     "start_time": "2024-03-26T20:50:52.382808Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RAG with initial state:\n",
      " {'__PRIOR_STEP': 'ai_converse',\n",
      " '__SEQUENCE_ID': 1,\n",
      " 'chat_history': ['Human: Who is Stefan? Please answer in English.',\n",
      "                  'AI: Stefan is someone who likes tacos, used to work at IBM, '\n",
      "                  'worked at Stitch Fix, and likes to bake sourdough.'],\n",
      " 'question': 'Who is Stefan? Please answer in English.'}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask something (or type exit to quit):  where does Elijah work?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳Processing input from user...\n",
      "🎙💬 where does Elijah work? \n",
      "\n",
      "🤔 AI is thinking...\n",
      "🤖💬 Elijah works at TwoSigma. \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask something (or type exit to quit):  what should I buy him to eat?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳Processing input from user...\n",
      "🎙💬 what should I buy him to eat? \n",
      "\n",
      "🤔 AI is thinking...\n",
      "🤖💬 You should buy Elijah some mangoes to eat. \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask something (or type exit to quit):  What type of mangoes does he like?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳Processing input from user...\n",
      "🎙💬 What type of mangoes does he like? \n",
      "\n",
      "🤔 AI is thinking...\n",
      "🤖💬 It is not specified what type of mangoes Elijah likes in the given context. \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask something (or type exit to quit):  Who is the most likely person to like corn & beans?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳Processing input from user...\n",
      "🎙💬 Who is the most likely person to like corn & beans? \n",
      "\n",
      "🤔 AI is thinking...\n",
      "🤖💬 Stefan is most likely to enjoy eating corn and beans. \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask something (or type exit to quit):  Why?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳Processing input from user...\n",
      "🎙💬 Why? \n",
      "\n",
      "🤔 AI is thinking...\n",
      "🤖💬 There is no clear indication in the context provided that Stefan would be the most likely person to enjoy eating corn and beans. The information suggests that Stefan enjoys tacos and baking sourdough, but there is nothing regarding his preferences for corn and beans specifically. \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask something (or type exit to quit):  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳Processing input from user...\n",
      "🎙💬 exit \n",
      "\n",
      "{'chat_history': ['Human: Who is Stefan? Please answer in English.',\n",
      "                  'AI: Stefan is someone who likes tacos, used to work at IBM, '\n",
      "                  'worked at Stitch Fix, and likes to bake sourdough.',\n",
      "                  'Human: where does Elijah work?',\n",
      "                  'AI: Elijah works at TwoSigma.',\n",
      "                  'Human: what should I buy him to eat?',\n",
      "                  'AI: You should buy Elijah some mangoes to eat.',\n",
      "                  'Human: What type of mangoes does he like?',\n",
      "                  'AI: It is not specified what type of mangoes Elijah likes '\n",
      "                  'in the given context.',\n",
      "                  'Human: Who is the most likely person to like corn & beans?',\n",
      "                  'AI: Stefan is most likely to enjoy eating corn and beans.',\n",
      "                  'Human: Why?',\n",
      "                  'AI: There is no clear indication in the context provided '\n",
      "                  'that Stefan would be the most likely person to enjoy eating '\n",
      "                  'corn and beans. The information suggests that Stefan enjoys '\n",
      "                  'tacos and baking sourdough, but there is nothing regarding '\n",
      "                  'his preferences for corn and beans specifically.',\n",
      "                  'Human: exit']}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Running RAG with initial state:\\n {pprint.pformat(app.state.get_all())}\")\n",
    "while True:\n",
    "    user_question = input(\"Ask something (or type exit to quit): \")\n",
    "    previous_action, result, state = app.run(\n",
    "        halt_before=[\"human_converse\"],\n",
    "        halt_after=[\"terminal\"],\n",
    "        inputs={\"user_question\": user_question},\n",
    "    )\n",
    "    if previous_action.name == \"terminal\":\n",
    "        # reached the end\n",
    "        pprint.pprint(result)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169946a65f977df9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Reloading from prior state\n",
    "\n",
    "Burr makes it easy to reload from a prior state. In this example we'll just use what is logged to the tracker to \"go back in time\" and reload the application to that state. \n",
    "\n",
    "This is useful for debugging, building the application itself, etc.\n",
    "\n",
    "There are two ways to load prior state:\n",
    "1. Load the state outside the Burr Application. i.e. pass it in as initial state.\n",
    "2. Use the ApplicationBuilder .initialize_from() method. You can optionally \"fork\" or continue an existing application this way.\n",
    "\n",
    "The difference between them, is that the first method is more flexible and gives you total control.\n",
    "The second method gives you two options: \n",
    "(a) to explicitly \"fork\" i.e. create a new application from a certain point in time from an existing application_id.\n",
    "(b) allows you to \"pick up where you left off\" with an existing application_id, e.g. in the case of a crash, or if you wanted to start from the last conversation with a user, and continue.\n",
    "\n",
    "Below we show how to do the first method. Then after that the second method, to show how to fork the conversation,\n",
    "as well as pick up the prior conversation from where it left off.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7f4dd64f73ed2d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T20:52:37.419848Z",
     "start_time": "2024-03-26T20:52:37.393869Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded state from app_id:63fce731-8cc8-4de0-b8dc-0a5f1a057a11, sequence_id:11::\n",
      " {'__SEQUENCE_ID': 11,\n",
      " 'chat_history': ['Human: Who is Stefan? Please answer in English.',\n",
      "                  'AI: Stefan is someone who likes tacos, used to work at IBM, '\n",
      "                  'worked at Stitch Fix, and likes to bake sourdough.',\n",
      "                  'Human: where does Elijah work?',\n",
      "                  'AI: Elijah works at TwoSigma.',\n",
      "                  'Human: what should I buy him to eat?',\n",
      "                  'AI: You should buy Elijah some mangoes to eat.',\n",
      "                  'Human: What type of mangoes does he like?',\n",
      "                  'AI: It is not specified what type of mangoes Elijah likes '\n",
      "                  'in the given context.',\n",
      "                  'Human: Who is the most likely person to like corn & beans?',\n",
      "                  'AI: Stefan is most likely to enjoy eating corn and beans.',\n",
      "                  'Human: Why?',\n",
      "                  'AI: There is no clear indication in the context provided '\n",
      "                  'that Stefan would be the most likely person to enjoy eating '\n",
      "                  'corn and beans. The information suggests that Stefan enjoys '\n",
      "                  'tacos and baking sourdough, but there is nothing regarding '\n",
      "                  'his preferences for corn and beans specifically.'],\n",
      " 'question': 'Why?'}\n"
     ]
    }
   ],
   "source": [
    "# set up for rewinding to a prior state -- loading it in as initial state\n",
    "prior_app_id = app_id\n",
    "last_sequence_id = app.sequence_id\n",
    "rewind_to_sequence_id = last_sequence_id - 2\n",
    "new_app_id = str(uuid.uuid4())\n",
    "\n",
    "project_name = \"demo:conversational-rag\"\n",
    "# we use the tracking client here to get the state of the application at a prior sequence_id\n",
    "tracker = LocalTrackingClient(project=project_name)\n",
    "persisted_state = tracker.load(partition_key=\"sample_user\", \n",
    "                               app_id=prior_app_id, \n",
    "                               sequence_id=rewind_to_sequence_id)\n",
    "state_values = persisted_state['state'].get_all()\n",
    "print(f\"Loaded state from app_id:{prior_app_id}, \"\n",
    "      f\"sequence_id:{rewind_to_sequence_id}::\\n \"\n",
    "      f\"{pprint.pformat(state_values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ee618e3-15c0-403b-bc96-3a2faaea457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build our other application using the state we just loaded.\n",
    "other_app = (\n",
    "    ApplicationBuilder()\n",
    "    # add the actions\n",
    "    .with_actions(\n",
    "        # bind the vector store to the AI conversational step\n",
    "        ai_converse=ai_converse.bind(vector_store=vector_store),\n",
    "        human_converse=human_converse,\n",
    "        terminal=burr.core.Result(\"chat_history\"),\n",
    "    )\n",
    "    # set the transitions between actions\n",
    "    .with_transitions(\n",
    "        (\"ai_converse\", \"human_converse\", default),\n",
    "        (\"human_converse\", \"terminal\", expr(\"'exit' in question\")),\n",
    "        (\"human_converse\", \"ai_converse\", default),\n",
    "    )\n",
    "    # add identifiers that will help track the application\n",
    "    .with_identifiers(app_id=new_app_id, partition_key=\"sample_user\")\n",
    "    # set state to prior state\n",
    "    .with_state(**persisted_state[\"state\"].get_all())\n",
    "    # say where we want to start\n",
    "    .with_entrypoint(\"human_converse\")\n",
    "    # add a hook to print the steps -- optional but shows that Burr is pluggable\n",
    "    .with_hooks(PrintStepHook())\n",
    "    # add tracking -- this will show up in the BURR UI.\n",
    "    .with_tracker(tracker)\n",
    "    # build the application\n",
    "    .build()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34140c5864b940dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T20:54:24.035153Z",
     "start_time": "2024-03-26T20:53:19.237522Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RAG with loaded state:\n",
      " {'__SEQUENCE_ID': 11,\n",
      " 'chat_history': ['Human: Who is Stefan? Please answer in English.',\n",
      "                  'AI: Stefan is someone who likes tacos, used to work at IBM, '\n",
      "                  'worked at Stitch Fix, and likes to bake sourdough.',\n",
      "                  'Human: where does Elijah work?',\n",
      "                  'AI: Elijah works at TwoSigma.',\n",
      "                  'Human: what should I buy him to eat?',\n",
      "                  'AI: You should buy Elijah some mangoes to eat.',\n",
      "                  'Human: What type of mangoes does he like?',\n",
      "                  'AI: It is not specified what type of mangoes Elijah likes '\n",
      "                  'in the given context.',\n",
      "                  'Human: Who is the most likely person to like corn & beans?',\n",
      "                  'AI: Stefan is most likely to enjoy eating corn and beans.',\n",
      "                  'Human: Why?',\n",
      "                  'AI: There is no clear indication in the context provided '\n",
      "                  'that Stefan would be the most likely person to enjoy eating '\n",
      "                  'corn and beans. The information suggests that Stefan enjoys '\n",
      "                  'tacos and baking sourdough, but there is nothing regarding '\n",
      "                  'his preferences for corn and beans specifically.'],\n",
      " 'question': 'Why?'}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask something (or type exit to quit):  does Harrison like pizza?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳Processing input from user...\n",
      "🎙💬 does Harrison like pizza? \n",
      "\n",
      "🤔 AI is thinking...\n",
      "🤖💬 There is no information provided in the context to determine whether Harrison likes pizza or not. \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask something (or type exit to quit):  Who should I take to a Mexican restaurant?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳Processing input from user...\n",
      "🎙💬 Who should I take to a Mexican restaurant? \n",
      "\n",
      "🤔 AI is thinking...\n",
      "🤖💬 Stefan \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask something (or type exit to quit):  Who worked at TwoSigma?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳Processing input from user...\n",
      "🎙💬 Who worked at TwoSigma? \n",
      "\n",
      "🤔 AI is thinking...\n",
      "🤖💬 Elijah worked at TwoSigma. \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask something (or type exit to quit):  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳Processing input from user...\n",
      "🎙💬 exit \n",
      "\n",
      "{'chat_history': ['Human: Who is Stefan? Please answer in English.',\n",
      "                  'AI: Stefan is someone who likes tacos, used to work at IBM, '\n",
      "                  'worked at Stitch Fix, and likes to bake sourdough.',\n",
      "                  'Human: where does Elijah work?',\n",
      "                  'AI: Elijah works at TwoSigma.',\n",
      "                  'Human: what should I buy him to eat?',\n",
      "                  'AI: You should buy Elijah some mangoes to eat.',\n",
      "                  'Human: What type of mangoes does he like?',\n",
      "                  'AI: It is not specified what type of mangoes Elijah likes '\n",
      "                  'in the given context.',\n",
      "                  'Human: Who is the most likely person to like corn & beans?',\n",
      "                  'AI: Stefan is most likely to enjoy eating corn and beans.',\n",
      "                  'Human: Why?',\n",
      "                  'AI: There is no clear indication in the context provided '\n",
      "                  'that Stefan would be the most likely person to enjoy eating '\n",
      "                  'corn and beans. The information suggests that Stefan enjoys '\n",
      "                  'tacos and baking sourdough, but there is nothing regarding '\n",
      "                  'his preferences for corn and beans specifically.',\n",
      "                  'Human: does Harrison like pizza?',\n",
      "                  'AI: There is no information provided in the context to '\n",
      "                  'determine whether Harrison likes pizza or not.',\n",
      "                  'Human: Who should I take to a Mexican restaurant?',\n",
      "                  'AI: Stefan',\n",
      "                  'Human: Who worked at TwoSigma?',\n",
      "                  'AI: Elijah worked at TwoSigma.',\n",
      "                  'Human: exit']}\n"
     ]
    }
   ],
   "source": [
    "# We can now change test, debug, etc. from this prior state.\n",
    "print(f\"Running RAG with loaded state:\\n {pprint.pformat(state_values)}\")\n",
    "while True:\n",
    "    user_question = input(\"Ask something (or type exit to quit): \")\n",
    "    previous_action, result, state = other_app.run(\n",
    "        halt_before=[\"human_converse\"],\n",
    "        halt_after=[\"terminal\"],\n",
    "        inputs={\"user_question\": user_question},\n",
    "    )\n",
    "    if previous_action and previous_action.name == \"terminal\":\n",
    "        # reached the end\n",
    "        pprint.pprint(result)\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7979bc-a09b-4abd-b22b-02ca38d4a26c",
   "metadata": {},
   "source": [
    "## Picking up where we left off from a prior conversation with `initialize_from()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc62a033644c7b80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T21:04:56.748649Z",
     "start_time": "2024-03-26T21:04:56.742019Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Now let's show how to use the ApplicationBuilder.initialize_from() method to pick up where we left off.\n",
    "# This is useful if you want to continue a conversation with a user, or if you had a crash, etc.\n",
    "\n",
    "# set up for rewinding to a prior state -- loading it in as initial state\n",
    "prior_app_id = app_id\n",
    "\n",
    "project_name = \"demo:conversational-rag\"\n",
    "# we use the tracking client here to get the state of the application at a prior sequence_id\n",
    "tracker = LocalTrackingClient(project=project_name)\n",
    "pick_up_where_we_left_off_app = (\n",
    "    ApplicationBuilder()\n",
    "    # add the actions\n",
    "    .with_actions(\n",
    "        # bind the vector store to the AI conversational step\n",
    "        ai_converse=ai_converse.bind(vector_store=vector_store),\n",
    "        human_converse=human_converse,\n",
    "        terminal=burr.core.Result(\"chat_history\"),\n",
    "    )\n",
    "    # set the transitions between actions\n",
    "    .with_transitions(\n",
    "        (\"ai_converse\", \"human_converse\", default),\n",
    "        (\"human_converse\", \"terminal\", expr(\"'exit' in question\")),\n",
    "        (\"human_converse\", \"ai_converse\", default),\n",
    "    )\n",
    "    # add identifiers that will help track the application\n",
    "    .with_identifiers(app_id=prior_app_id, partition_key=\"sample_user\")\n",
    "    .initialize_from(\n",
    "        initializer=tracker,\n",
    "        resume_at_next_action=False, # we want to always start at human_converse; our entrypoint\n",
    "        default_entrypoint=\"human_converse\",\n",
    "        default_state=initial_state, # set some default state incase we can't find the prior state\n",
    "    )\n",
    "    # add a hook to print the steps -- optional but shows that Burr is pluggable\n",
    "    .with_hooks(PrintStepHook())\n",
    "    # add tracking -- this will show up in the BURR UI.\n",
    "    .with_tracker(tracker)\n",
    "    # build the application\n",
    "    .build()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6d23d6d6a6643d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T21:05:41.246005Z",
     "start_time": "2024-03-26T21:05:02.855430Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RAG with loaded state:\n",
      " {'__PRIOR_STEP': 'terminal',\n",
      " '__SEQUENCE_ID': 13,\n",
      " 'chat_history': ['Human: Who is Stefan? Please answer in English.',\n",
      "                  'AI: Stefan is someone who likes tacos, used to work at IBM, '\n",
      "                  'worked at Stitch Fix, and likes to bake sourdough.',\n",
      "                  'Human: where does Elijah work?',\n",
      "                  'AI: Elijah works at TwoSigma.',\n",
      "                  'Human: what should I buy him to eat?',\n",
      "                  'AI: You should buy Elijah some mangoes to eat.',\n",
      "                  'Human: What type of mangoes does he like?',\n",
      "                  'AI: It is not specified what type of mangoes Elijah likes '\n",
      "                  'in the given context.',\n",
      "                  'Human: Who is the most likely person to like corn & beans?',\n",
      "                  'AI: Stefan is most likely to enjoy eating corn and beans.',\n",
      "                  'Human: Why?',\n",
      "                  'AI: There is no clear indication in the context provided '\n",
      "                  'that Stefan would be the most likely person to enjoy eating '\n",
      "                  'corn and beans. The information suggests that Stefan enjoys '\n",
      "                  'tacos and baking sourdough, but there is nothing regarding '\n",
      "                  'his preferences for corn and beans specifically.',\n",
      "                  'Human: exit'],\n",
      " 'question': 'exit'}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask something (or type exit to quit):  Tell me what you know about Harrison?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳Processing input from user...\n",
      "🎙💬 Tell me what you know about Harrison? \n",
      "\n",
      "🤔 AI is thinking...\n",
      "🤖💬 Based on the context provided, I know that Harrison worked at Kensho. \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask something (or type exit to quit):  What about Elijah?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳Processing input from user...\n",
      "🎙💬 What about Elijah? \n",
      "\n",
      "🤔 AI is thinking...\n",
      "🤖💬 Elijah enjoys biking, likes mango, and has worked at TwoSigma. \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask something (or type exit to quit):  What can you tell me about TwoSigma?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳Processing input from user...\n",
      "🎙💬 What can you tell me about TwoSigma? \n",
      "\n",
      "🤔 AI is thinking...\n",
      "🤖💬 Elijah worked at TwoSigma. \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask something (or type exit to quit):  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳Processing input from user...\n",
      "🎙💬 quit \n",
      "\n",
      "🤔 AI is thinking...\n",
      "🤖💬 There is no information provided indicating whether or not the conversation was ended. \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask something (or type exit to quit):  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳Processing input from user...\n",
      "🎙💬 exit \n",
      "\n",
      "{'chat_history': ['Human: Who is Stefan? Please answer in English.',\n",
      "                  'AI: Stefan is someone who likes tacos, used to work at IBM, '\n",
      "                  'worked at Stitch Fix, and likes to bake sourdough.',\n",
      "                  'Human: where does Elijah work?',\n",
      "                  'AI: Elijah works at TwoSigma.',\n",
      "                  'Human: what should I buy him to eat?',\n",
      "                  'AI: You should buy Elijah some mangoes to eat.',\n",
      "                  'Human: What type of mangoes does he like?',\n",
      "                  'AI: It is not specified what type of mangoes Elijah likes '\n",
      "                  'in the given context.',\n",
      "                  'Human: Who is the most likely person to like corn & beans?',\n",
      "                  'AI: Stefan is most likely to enjoy eating corn and beans.',\n",
      "                  'Human: Why?',\n",
      "                  'AI: There is no clear indication in the context provided '\n",
      "                  'that Stefan would be the most likely person to enjoy eating '\n",
      "                  'corn and beans. The information suggests that Stefan enjoys '\n",
      "                  'tacos and baking sourdough, but there is nothing regarding '\n",
      "                  'his preferences for corn and beans specifically.',\n",
      "                  'Human: exit',\n",
      "                  'Human: Tell me what you know about Harrison?',\n",
      "                  'AI: Based on the context provided, I know that Harrison '\n",
      "                  'worked at Kensho.',\n",
      "                  'Human: What about Elijah?',\n",
      "                  'AI: Elijah enjoys biking, likes mango, and has worked at '\n",
      "                  'TwoSigma.',\n",
      "                  'Human: What can you tell me about TwoSigma?',\n",
      "                  'AI: Elijah worked at TwoSigma.',\n",
      "                  'Human: quit',\n",
      "                  'AI: There is no information provided indicating whether or '\n",
      "                  'not the conversation was ended.',\n",
      "                  'Human: exit']}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Running RAG with loaded state:\\n {pprint.pformat(pick_up_where_we_left_off_app.state.get_all())}\")\n",
    "while True:\n",
    "    user_question = input(\"Ask something (or type exit to quit): \")\n",
    "    previous_action, result, state = pick_up_where_we_left_off_app.run(\n",
    "        halt_before=[\"human_converse\"],\n",
    "        halt_after=[\"terminal\"],\n",
    "        inputs={\"user_question\": user_question},\n",
    "    )\n",
    "    if previous_action and previous_action.name == \"terminal\":\n",
    "        # reached the end\n",
    "        pprint.pprint(result)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51159c1-04a1-4b55-8615-97dc6d5c6975",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Forking the application using prior state with `initialize_from()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84d533ba-fd85-4b67-a6c3-30356cc9b048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's show how to use the ApplicationBuilder.initialize_from() method to fork a new application.\n",
    "# This is useful if you want to debug/reuse prior state, rewind and try something else, etc.\n",
    "\n",
    "# set up for rewinding to a prior state -- loading it in as initial state\n",
    "prior_app_id = app_id\n",
    "new_app_id = str(uuid.uuid4())\n",
    "\n",
    "project_name = \"demo:conversational-rag\"\n",
    "# we use the tracking client here to get the state of the application at a prior sequence_id\n",
    "tracker = LocalTrackingClient(project=project_name)\n",
    "forking_new_application = (\n",
    "    ApplicationBuilder()\n",
    "    # add the actions\n",
    "    .with_actions(\n",
    "        # bind the vector store to the AI conversational step\n",
    "        ai_converse=ai_converse.bind(vector_store=vector_store),\n",
    "        human_converse=human_converse,\n",
    "        terminal=burr.core.Result(\"chat_history\"),\n",
    "    )\n",
    "    # set the transitions between actions\n",
    "    .with_transitions(\n",
    "        (\"ai_converse\", \"human_converse\", default),\n",
    "        (\"human_converse\", \"terminal\", expr(\"'exit' in question\")),\n",
    "        (\"human_converse\", \"ai_converse\", default),\n",
    "    )\n",
    "    # add identifiers that will help track the application\n",
    "    .with_identifiers(app_id=new_app_id, partition_key=\"sample_user\")\n",
    "    .initialize_from(\n",
    "        initializer=tracker,\n",
    "        resume_at_next_action=False, # we want to always start at human_converse; our entrypoint\n",
    "        default_entrypoint=\"human_converse\",\n",
    "        default_state=initial_state, # set some default state incase we can't find the prior state\n",
    "        fork_from_app_id=prior_app_id, # <---- The new addition \n",
    "        fork_from_partition_key=\"sample_user\",  # <---- The new addition \n",
    "        fork_from_sequence_id=5,  # <---- The new addition \n",
    "    )\n",
    "    # add a hook to print the steps -- optional but shows that Burr is pluggable\n",
    "    .with_hooks(PrintStepHook())\n",
    "    # add tracking -- this will show up in the BURR UI.\n",
    "    .with_tracker(tracker)\n",
    "    # build the application\n",
    "    .build()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9086e58-eeaa-4886-a113-5c7fd5fe0af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RAG with loaded state:\n",
      " {'__SEQUENCE_ID': 5,\n",
      " 'chat_history': ['Human: Who is Stefan? Please answer in English.',\n",
      "                  'AI: Stefan is someone who likes tacos, used to work at IBM, '\n",
      "                  'worked at Stitch Fix, and likes to bake sourdough.',\n",
      "                  'Human: where does Elijah work?',\n",
      "                  'AI: Elijah works at TwoSigma.',\n",
      "                  'Human: what should I buy him to eat?',\n",
      "                  'AI: You should buy Elijah some mangoes to eat.'],\n",
      " 'question': 'what should I buy him to eat?'}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask something (or type exit to quit):  who should I go on a bike ride with?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳Processing input from user...\n",
      "🎙💬 who should I go on a bike ride with? \n",
      "\n",
      "🤔 AI is thinking...\n",
      "🤖💬 You should go on a bike ride with Elijah. \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask something (or type exit to quit):  I have too much flour, who do you think I should gift it to?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳Processing input from user...\n",
      "🎙💬 I have too much flour, who do you think I should gift it to? \n",
      "\n",
      "🤔 AI is thinking...\n",
      "🤖💬 Based on the context provided, Stefan seems like the best candidate to gift your excess flour to as he enjoys baking sourdough. \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask something (or type exit to quit):  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳Processing input from user...\n",
      "🎙💬 exit \n",
      "\n",
      "{'chat_history': ['Human: Who is Stefan? Please answer in English.',\n",
      "                  'AI: Stefan is someone who likes tacos, used to work at IBM, '\n",
      "                  'worked at Stitch Fix, and likes to bake sourdough.',\n",
      "                  'Human: where does Elijah work?',\n",
      "                  'AI: Elijah works at TwoSigma.',\n",
      "                  'Human: what should I buy him to eat?',\n",
      "                  'AI: You should buy Elijah some mangoes to eat.',\n",
      "                  'Human: who should I go on a bike ride with?',\n",
      "                  'AI: You should go on a bike ride with Elijah.',\n",
      "                  'Human: I have too much flour, who do you think I should '\n",
      "                  'gift it to?',\n",
      "                  'AI: Based on the context provided, Stefan seems like the '\n",
      "                  'best candidate to gift your excess flour to as he enjoys '\n",
      "                  'baking sourdough.',\n",
      "                  'Human: exit']}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Running RAG with loaded state:\\n {pprint.pformat(forking_new_application.state.get_all())}\")\n",
    "while True:\n",
    "    user_question = input(\"Ask something (or type exit to quit): \")\n",
    "    previous_action, result, state = forking_new_application.run(\n",
    "        halt_before=[\"human_converse\"],\n",
    "        halt_after=[\"terminal\"],\n",
    "        inputs={\"user_question\": user_question},\n",
    "    )\n",
    "    if previous_action and previous_action.name == \"terminal\":\n",
    "        # reached the end\n",
    "        pprint.pprint(result)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6523a54-e333-4d5e-941d-d3b28b940500",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LocalTrackingClient returned None while trying to fork from: partition_key:None, app_id:NO_SUCH_APP_ID, sequence_id:None. You explicitly requested to fork from a prior application run, but it does not exist. Defaulting to state defaults instead.\n"
     ]
    }
   ],
   "source": [
    "# this should print some warnings:\n",
    "new_app_id = str(uuid.uuid4())\n",
    "\n",
    "project_name = \"demo:conversational-rag\"\n",
    "# we use the tracking client here to get the state of the application at a prior sequence_id\n",
    "tracker = LocalTrackingClient(project=project_name)\n",
    "bad_fork = (\n",
    "    ApplicationBuilder()\n",
    "    # add the actions\n",
    "    .with_actions(\n",
    "        # bind the vector store to the AI conversational step\n",
    "        ai_converse=ai_converse.bind(vector_store=vector_store),\n",
    "        human_converse=human_converse,\n",
    "        terminal=burr.core.Result(\"chat_history\"),\n",
    "    )\n",
    "    # set the transitions between actions\n",
    "    .with_transitions(\n",
    "        (\"ai_converse\", \"human_converse\", default),\n",
    "        (\"human_converse\", \"terminal\", expr(\"'exit' in question\")),\n",
    "        (\"human_converse\", \"ai_converse\", default),\n",
    "    )\n",
    "    # add identifiers that will help track the application\n",
    "    .with_identifiers(app_id=new_app_id, partition_key=\"sample_user\")\n",
    "    .initialize_from(\n",
    "        initializer=tracker,\n",
    "        resume_at_next_action=False, # we want to always start at human_converse; our entrypoint\n",
    "        default_entrypoint=\"human_converse\",\n",
    "        default_state=initial_state, # set some default state incase we can't find the prior state\n",
    "        fork_from_app_id=\"NO_SUCH_APP_ID\", # <---- The new addition \n",
    "    )\n",
    "    # add a hook to print the steps -- optional but shows that Burr is pluggable\n",
    "    .with_hooks(PrintStepHook())\n",
    "    # add tracking -- this will show up in the BURR UI.\n",
    "    .with_tracker(tracker)\n",
    "    # build the application\n",
    "    .build()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd69381-1a63-43c4-abf0-bfcca2d695a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
