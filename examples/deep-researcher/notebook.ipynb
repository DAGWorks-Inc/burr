{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595fafdc-5e12-4ab7-8e21-18e49c453340",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install burr[start]\n",
    "!pip install openai\n",
    "!pip install tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed859b06-bcd5-4a6b-abcd-ab5863e17b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from application import application as researcher_app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d778f6-79e6-4a97-9360-48745f918c78",
   "metadata": {},
   "source": [
    "## Deep Researcher\n",
    "\n",
    "This is an example of a research assistant agent that uses llm chat and llm-friendly search together to provide a report, complete with internet citations, on a topic of the user's choosing. The graph for the agent includes optional reflection and re-writing that is implemented as a loop. This particular version of a research assistant uses the OpenAI API code for chat and the Tavily API for search. \n",
    "\n",
    "Given a research topic, the research assistant flow consists of several steps. On initialization, the assistant must:\n",
    "\n",
    "1. Use the chat API to generate a search query for web search based on the research topic.\n",
    "\n",
    "It then loops through the following three steps as many times as the user desires, controlled by the variable `num_loops`.\n",
    "\n",
    "2. Perform web research based on the search query using the search API.\n",
    "3. Use the chat API to summarize the web research gathered in the previous step.\n",
    "4. Reflect on the summary to generate a follow-up search query.\n",
    "\n",
    "When the number of loops is exhausted, the assistant must:\n",
    "\n",
    "5. Finalize summary with formatted sources and return it to the user.\n",
    "\n",
    "The graph for the application is visualized below.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d7c2c9-7f05-4447-b084-88bdd73e75f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_research_topic = \"getting a job in datascience\"\n",
    "app = researcher_app(app_id=\"1\", num_loops=2, research_topic=my_research_topic)\n",
    "app.visualize(include_conditions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1601d582-1a0b-435e-8f92-ffc7a44d4ca4",
   "metadata": {},
   "source": [
    "To run the application, we need tokens for the OpenAI api and the Tavily api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a899bf02-7f54-4cf2-ad80-bf37b62f07e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-*\"\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"tvly-*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e18b2ad-4696-4c15-84b6-04c3ba515d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "action, state, result = app.run(halt_after=[\"finalize_summary\"])\n",
    "print(app.state[\"running_summary\"])     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
